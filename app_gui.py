"""
Clone Voice Adapter Studio - Main GUI
Pipeline: Record ‚Üí Cut Audio ‚Üí Typhoon STT ‚Üí Prepare Data ‚Üí Train ‚Üí Inference
"""
import gradio as gr
import os
import tempfile
import zipfile
import shutil
from pathlib import Path
from datetime import datetime
import threading

# Local modules
from audio_processor import process_audio_for_dataset, get_audio_duration
from typhoon_stt import process_wavs_to_metadata, transcribe_all_segments, save_metadata_csv
from train import train
from inference import InferenceEngine

# ===============================
# HISTORY SYSTEM
# ===============================
HISTORY_DIR = Path(__file__).parent / "output_history"
HISTORY_DIR.mkdir(parents=True, exist_ok=True)


# ===============================
# TAB 1: RECORD & CREATE DATASET
# ===============================
def process_recording_pipeline(audio_path, typhoon_key, min_sec, max_sec, progress=gr.Progress()):
    """
    Full pipeline:
    1. ‡∏ï‡∏±‡∏î audio ‡πÄ‡∏õ‡πá‡∏ô 11-15s segments, ‡∏•‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö
    2. ‡∏™‡πà‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ segment ‡πÑ‡∏õ Typhoon STT
    3. ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata.csv (LJSpeech format)
    4. ‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP file
    """
    if not audio_path:
        return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", "", "", None
    
    if not typhoon_key:
        return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Typhoon API Key", "", "", None
    
    try:
        audio_path = Path(audio_path)
        basename = audio_path.stem
        temp_dir = Path(tempfile.mkdtemp(prefix="voice_clone_"))
        
        # Step 1: Process audio (remove silence, cut segments)
        progress(0.1, desc="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á...")
        print(f"[Pipeline] Step 1: Processing audio...")
        segment_paths = process_audio_for_dataset(
            str(audio_path), 
            str(temp_dir),
            min_sec=int(min_sec),
            max_sec=int(max_sec)
        )
        
        if not segment_paths:
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ", "", "", None
        
        # Calculate total duration
        total_segments = len(segment_paths)
        segments_info = f"‡∏ï‡∏±‡∏î‡πÑ‡∏î‡πâ {total_segments} segments"
        
        # Step 2: Transcribe with Typhoon
        progress(0.3, desc="‡∏Å‡∏≥‡∏•‡∏±‡∏á transcribe ‡∏î‡πâ‡∏ß‡∏¢ Typhoon...")
        print(f"[Pipeline] Step 2: Transcribing {total_segments} segments...")
        
        wavs_dir = temp_dir / "wavs"
        
        def update_progress(current, total, filename):
            progress(0.3 + (0.5 * current / total), desc=f"Transcribing {filename}...")
        
        transcriptions = transcribe_all_segments(
            str(wavs_dir), 
            typhoon_key,
            progress_callback=update_progress
        )
        
        if not transcriptions:
            return "‚ùå Typhoon ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ transcribe ‡πÑ‡∏î‡πâ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key)", segments_info, "", None
        
        # Step 3: Save metadata.csv (LJSpeech format)
        progress(0.85, desc="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á metadata...")
        print(f"[Pipeline] Step 3: Saving metadata...")
        
        metadata_path = temp_dir / "metadata.csv"
        save_metadata_csv(transcriptions, str(metadata_path), format_type="ljspeech")
        
        # Read metadata content for display
        metadata_content = metadata_path.read_text(encoding="utf-8")
        
        # Step 4: Create ZIP
        progress(0.95, desc="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP...")
        print(f"[Pipeline] Step 4: Creating ZIP...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_path = temp_dir / f"{basename}_dataset_{timestamp}.zip"
        
        with zipfile.ZipFile(zip_path, "w") as zf:
            zf.write(metadata_path, "metadata.csv")
            for wav_file in wavs_dir.glob("*.wav"):
                zf.write(wav_file, f"wavs/{wav_file.name}")
        
        # Copy to history
        history_zip = HISTORY_DIR / zip_path.name
        shutil.copy(zip_path, history_zip)
        
        # Also save dataset folder for training
        dataset_dir = Path(__file__).parent / "dataset"
        if dataset_dir.exists():
            shutil.rmtree(dataset_dir)
        shutil.copytree(temp_dir, dataset_dir)
        
        progress(1.0, desc="‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        
        # Create preview
        preview_lines = []
        for i, (filename, text) in enumerate(transcriptions[:10], 1):
            preview_lines.append(f"[{i}] {text[:60]}...")
        preview = "\n".join(preview_lines)
        
        status = f"‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏™‡∏£‡πâ‡∏≤‡∏á {len(transcriptions)} segments\nüìÅ Dataset ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {dataset_dir}"
        
        return status, preview, metadata_content, str(zip_path)
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"‚ùå Error: {str(e)}", "", "", None


# ===============================
# TAB 2: TRAINING
# ===============================
def trigger_training(metadata_path, max_steps, learning_rate):
    """Start training in background thread"""
    try:
        if not os.path.exists(metadata_path):
            return f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {metadata_path}"
        
        def run_train():
            train(metadata_path, overrides={
                "max_steps": int(max_steps), 
                "learning_rate": float(learning_rate)
            })
        
        t = threading.Thread(target=run_train)
        t.start()
        
        return "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Training ‡πÅ‡∏•‡πâ‡∏ß! ‡∏î‡∏π progress ‡πÉ‡∏ô console..."
        
    except Exception as e:
        return f"‚ùå Training Error: {str(e)}"


# ===============================
# TAB 3: INFERENCE
# ===============================
inference_engine = None

def run_inference(text, model_path):
    """Generate audio from text"""
    global inference_engine
    
    if not text.strip():
        return None, "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"
    
    if not os.path.exists(model_path):
        return None, f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö model ‡∏ó‡∏µ‡πà {model_path}"
    
    try:
        if inference_engine is None or inference_engine.model_path != model_path:
            print(f"[Inference] Loading model from {model_path}...")
            inference_engine = InferenceEngine(model_path=model_path)
        
        output_path = inference_engine.generate(text)
        return output_path, "‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"‚ùå Inference Error: {str(e)}"


# ===============================
# GRADIO UI
# ===============================
with gr.Blocks(title="Clone Voice Adapter Studio", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üéôÔ∏è Clone Voice Adapter Studio
    **Pipeline:** ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‚Üí ‡∏ï‡∏±‡∏î 11-15s ‚Üí Typhoon STT ‚Üí Train ‚Üí Inference
    """)

    with gr.Tabs():
        # =====================
        # TAB 1: RECORD & PREPARE
        # =====================
        with gr.Tab("1. üé§ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å & ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset"):
            gr.Markdown("""
            ### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
            1. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á **1-5 ‡∏ô‡∏≤‡∏ó‡∏µ** (‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏ó‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)
            2. ‡πÉ‡∏™‡πà **Typhoon API Key**
            3. ‡∏Å‡∏î **‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset**
            """)
            
            # Reading script
            with gr.Accordion("üìñ ‡∏ö‡∏ó‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π)", open=False):
                gr.Markdown("""
                **‡∏ö‡∏ó‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡πâ‡∏≤‡πÜ ‡∏ä‡∏±‡∏î‡πÜ ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3 ‡∏ô‡∏≤‡∏ó‡∏µ):**
                
                ---
                
                "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
                ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≤‡∏®‡∏±‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á

                ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ AI ‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô
                ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πà‡∏≤‡∏ß
                ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏Ñ‡∏•‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏ô‡∏±‡πâ‡∏ô ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢

                ‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Orpheus ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• text-to-speech ‡∏ó‡∏µ‡πà‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢
                ‡∏ú‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ä‡πâ‡∏≤‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

                ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏£‡πà‡∏ß‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö"
                
                ---
                """)
            
            with gr.Row():
                with gr.Column(scale=2):
                    rec_audio = gr.Audio(
                        sources=["microphone", "upload"], 
                        type="filepath", 
                        label="üî¥ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡∏´‡∏£‡∏∑‡∏≠ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå"
                    )
                with gr.Column(scale=1):
                    rec_api_key = gr.Textbox(
                        label="üîë Typhoon API Key", 
                        type="password",
                        placeholder="sk-..."
                    )
            
            with gr.Row():
                rec_min_sec = gr.Slider(8, 15, value=11, step=1, label="Min Segment (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)")
                rec_max_sec = gr.Slider(12, 20, value=15, step=1, label="Max Segment (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)")
            
            rec_process_btn = gr.Button("üöÄ ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset", variant="primary", size="lg")
            
            with gr.Row():
                rec_status = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", lines=3)
                rec_preview = gr.Textbox(label="Preview Transcriptions", lines=5)
            
            with gr.Row():
                rec_metadata = gr.Textbox(label="metadata.csv", lines=5)
                rec_zip = gr.File(label="üì¶ Download Dataset ZIP")
            
            rec_process_btn.click(
                process_recording_pipeline,
                inputs=[rec_audio, rec_api_key, rec_min_sec, rec_max_sec],
                outputs=[rec_status, rec_preview, rec_metadata, rec_zip]
            )

        # =====================
        # TAB 2: TRAIN
        # =====================
        with gr.Tab("2. üèãÔ∏è Train Model"):
            gr.Markdown("""
            ### Fine-tune LoRA Model
            ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Train ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fine-tune model
            loss ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 0.5 - 0.05 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏µ
            """)
            
            train_meta_path = gr.Textbox(
                label="üìÅ Path to metadata.csv", 
                value="dataset/metadata.csv"
            )
            
            with gr.Row():
                train_steps = gr.Number(label="Max Steps", value=600)
                train_lr = gr.Number(label="Learning Rate", value=2e-4)
            
            train_btn = gr.Button("üöÄ Start Training", variant="primary")
            train_status = gr.Textbox(label="Training Status", lines=3)
            
            train_btn.click(
                trigger_training, 
                inputs=[train_meta_path, train_steps, train_lr], 
                outputs=train_status
            )
            
            gr.Markdown("""
            > ‚ö†Ô∏è **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** Training ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 10-30 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô steps ‡πÅ‡∏•‡∏∞ GPU
            > ‡∏î‡∏π progress ‡πÑ‡∏î‡πâ‡πÉ‡∏ô terminal/console
            """)

        # =====================
        # TAB 3: INFERENCE
        # =====================
        with gr.Tab("3. üîä Inference"):
            gr.Markdown("""
            ### ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å Text
            ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏π‡∏î
            """)
            
            inf_model_path = gr.Textbox(
                label="üìÅ Model Path", 
                value="outputs/lora_model"
            )
            inf_text = gr.Textbox(
                label="üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î", 
                lines=3,
                placeholder="‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å AI..."
            )
            
            inf_btn = gr.Button("üéµ Generate Audio", variant="primary")
            
            with gr.Row():
                inf_audio = gr.Audio(label="üîä Generated Audio")
                inf_status = gr.Textbox(label="Status")
            
            inf_btn.click(
                run_inference, 
                inputs=[inf_text, inf_model_path], 
                outputs=[inf_audio, inf_status]
            )


if __name__ == "__main__":
    demo.launch(share=True)
